{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c4dbf2-087e-46f8-babf-83aed83a1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Identifier']\n"
     ]
    }
   ],
   "source": [
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "\n",
    "# Load schema\n",
    "sv = SchemaView(\"name-schema.yaml\")\n",
    "\n",
    "# Get all non-abstract classes (use .abstract instead of .get(\"abstract\"))\n",
    "concrete_classes = [cls for cls in sv.all_classes().values() if not cls.abstract]\n",
    "print([cls.name for cls in concrete_classes])  # should show: ['Name', 'Identifier']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5938e66-ea42-4586-b3e7-1ed13fdab8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def map_range_to_spark_type(slot_range: str, multivalued: bool = False):\n",
    "    type_mapping = {\n",
    "        \"string\": StringType(),\n",
    "        \"float\": FloatType(),\n",
    "        \"integer\": IntegerType(),\n",
    "        \"boolean\": BooleanType(),\n",
    "        \"UUID\": StringType(),\n",
    "        \"uriorcurie\": StringType()\n",
    "    }\n",
    "    data_type = type_mapping.get(slot_range, StringType())\n",
    "    return ArrayType(data_type) if multivalued else data_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f296e1-12a0-4dda-8e3c-b34d19242c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.4.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8bb7bdf9-294b-4f07-8dbc-54ff916fc402;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 276ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8bb7bdf9-294b-4f07-8dbc-54ff916fc402\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "25/05/20 15:46:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 15:46:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaTableFromLinkML\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def build_schema_and_create_table(class_name: str, output_path: str):\n",
    "    cls = sv.get_class(class_name)\n",
    "    attrs = sv.class_slots(class_name)\n",
    "    \n",
    "    fields = []\n",
    "    for attr in attrs:\n",
    "        slot = sv.induced_slot(attr, class_name)\n",
    "        dtype = map_range_to_spark_type(slot.range or \"string\", slot.multivalued)\n",
    "        nullable = not slot.required\n",
    "        fields.append(StructField(attr, dtype, nullable))\n",
    "    \n",
    "    schema = StructType(fields)\n",
    "    \n",
    "    # Create empty DataFrame with schema and save as Delta table\n",
    "    df = spark.createDataFrame([], schema)\n",
    "    delta_path = os.path.join(output_path, class_name.lower())\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "    \n",
    "    print(f\"Delta table for {class_name} created at {delta_path}\")\n",
    "    return delta_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88e12f2-b41a-4c06-ac2a-45ec963ddcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 15:46:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table for Name created at /tmp/delta/name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=====================================================>  (48 + 2) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table for Identifier created at /tmp/delta/identifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "base_path = \"/tmp/delta\"  # change if needed\n",
    "name_path = build_schema_and_create_table(\"Name\", base_path)\n",
    "identifier_path = build_schema_and_create_table(\"Identifier\", base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3bd2da-e65f-4881-bd69-f9d789f9b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table for Name created at /tmp/delta/name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:==================================================>     (45 + 2) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table for Identifier created at /tmp/delta/identifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "base_path = \"/tmp/delta\"\n",
    "created_table_paths = {}\n",
    "\n",
    "# Iterate over all non-abstract (concrete) classes\n",
    "for cls in sv.all_classes().values():\n",
    "    if not cls.abstract:\n",
    "        class_name = cls.name\n",
    "        path = build_schema_and_create_table(class_name, base_path)\n",
    "        created_table_paths[class_name] = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770577a0-15a6-4fda-a14a-23ce3d9eae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Name dummy data\n",
    "name_data = [\n",
    "    (\"Protein A\", \"uuid-1\", \"Heat-inducible transcription repressor HrcA\", \"NCBI\"),\n",
    "    (\"Protein B\", \"uuid-2\", \"Uncharacterized protein 002R\", \"TrEMBL\")\n",
    "]\n",
    "name_schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True)\n",
    "])\n",
    "name_df = spark.createDataFrame(name_data, name_schema)\n",
    "name_df.write.format(\"delta\").mode(\"append\").save(name_path)\n",
    "\n",
    "# Identifier dummy data\n",
    "identifier_data = [\n",
    "    (\"uuid-1\", \"UniProt:Q8KCD6\", \"Protein A ID\", \"UniProt\"),\n",
    "    (\"uuid-2\", \"EC:5.2.3.14\", \"Protein B ID\", \"NCBI\")\n",
    "]\n",
    "identifier_schema = StructType([\n",
    "    StructField(\"entity_id\", StringType(), False),\n",
    "    StructField(\"identifier\", StringType(), False),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True)\n",
    "])\n",
    "identifier_df = spark.createDataFrame(identifier_data, identifier_schema)\n",
    "identifier_df.write.format(\"delta\").mode(\"append\").save(identifier_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec09cf4f-164e-472b-aeab-c2328cbd5edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Schema for table at /tmp/delta/name:\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      "\n",
      "📊 Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+---------+---------+------+\n",
      "|description                                |entity_id|name     |source|\n",
      "+-------------------------------------------+---------+---------+------+\n",
      "|Heat-inducible transcription repressor HrcA|uuid-1   |Protein A|NCBI  |\n",
      "|Uncharacterized protein 002R               |uuid-2   |Protein B|TrEMBL|\n",
      "+-------------------------------------------+---------+---------+------+\n",
      "\n",
      "\n",
      "📄 Schema for table at /tmp/delta/identifier:\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- identifier: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      "\n",
      "📊 Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------+-------+\n",
      "|description |entity_id|identifier    |source |\n",
      "+------------+---------+--------------+-------+\n",
      "|Protein A ID|uuid-1   |UniProt:Q8KCD6|UniProt|\n",
      "|Protein B ID|uuid-2   |EC:5.2.3.14   |NCBI   |\n",
      "+------------+---------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_table(path):\n",
    "    print(f\"\\n📄 Schema for table at {path}:\")\n",
    "    df = spark.read.format(\"delta\").load(path)\n",
    "    df.printSchema()\n",
    "    print(\"📊 Data:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "# Show all created tables\n",
    "for class_name, path in created_table_paths.items():\n",
    "    show_table(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6571be-46dc-4c22-b808-d8aa85ea3031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
